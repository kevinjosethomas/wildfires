{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8208775,"sourceType":"datasetVersion","datasetId":4864341}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport keras\nimport numpy as np\nfrom keras import ops\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras import layers\nfrom tensorflow import data as tf_data\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nkeras.utils.set_random_seed(seed=42)\n\nprint(keras.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-01T20:46:51.193380Z","iopub.execute_input":"2024-07-01T20:46:51.193813Z","iopub.status.idle":"2024-07-01T20:46:51.208232Z","shell.execute_reply.started":"2024-07-01T20:46:51.193776Z","shell.execute_reply":"2024-07-01T20:46:51.207006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_dataset():\n    train_points = []\n    train_labels = []\n#     test_points = []\n#     test_labels = []\n    class_map = {}\n    folders = glob.glob(\"/kaggle/input/sign-points/dataset/\")\n    \n    letters = \"ABCDEFGHIKLMNOPQRSTUVWXY\"\n    \n    for i, letter in enumerate(letters):\n        print(f\"Processing class: {letter} {i}\")\n        \n        train_directory = f\"/kaggle/input/sign-points/dataset/{letter}\"\n#         test_directory = f\"/kaggle/input/sign-points/dataset/{letter}\"\n        \n        class_map[i] = letter\n        train_files = glob.glob(train_directory + \"/*\")\n#         test_files = glob.glob(test_directory + \"/*\")\n        \n        for train_file in train_files:\n            train_points.append(np.load(train_file))\n            train_labels.append(i)\n            \n#         for test_file in test_files:\n#             test_points.append(np.load(test_file))\n#             test_labels.append(i)\n            \n        print(len(train_points))\n        print(len(train_labels))\n\n    return (\n        np.array(train_points),\n#         np.array(test_points),\n        np.array(train_labels),\n#         np.array(test_labels),\n        class_map,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:46:51.210281Z","iopub.execute_input":"2024-07-01T20:46:51.210634Z","iopub.status.idle":"2024-07-01T20:46:51.224243Z","shell.execute_reply.started":"2024-07-01T20:46:51.210607Z","shell.execute_reply":"2024-07-01T20:46:51.223032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_POINTS = 21\nNUM_CLASSES = 24\nBATCH_SIZE = 64\n\ntrain_points, train_labels, CLASS_MAP = parse_dataset()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:46:51.226951Z","iopub.execute_input":"2024-07-01T20:46:51.227519Z","iopub.status.idle":"2024-07-01T20:48:33.243985Z","shell.execute_reply.started":"2024-07-01T20:46:51.227489Z","shell.execute_reply":"2024-07-01T20:48:33.242874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = tf_data.Dataset.from_tensor_slices((test_points, test_labels))\n# test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)\n\ndataset = tf_data.Dataset.from_tensor_slices((train_points, train_labels))\n\ntrain_size = int(len(dataset) * 0.75)\nvalidation_size = int(len(dataset) * 0.20)\ntest_size = int(len(dataset) * 0.05)\n\ndataset = dataset.shuffle(len(train_points))\n\ntrain_dataset = dataset.take(train_size).batch(BATCH_SIZE)\nvalidation_dataset = dataset.skip(train_size).take(validation_size).batch(BATCH_SIZE)\ntest_dataset = dataset.skip(train_size+validation_size).take(test_size).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:48:33.245871Z","iopub.execute_input":"2024-07-01T20:48:33.246211Z","iopub.status.idle":"2024-07-01T20:48:33.458309Z","shell.execute_reply.started":"2024-07-01T20:48:33.246183Z","shell.execute_reply":"2024-07-01T20:48:33.457479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_bn(x, filters):\n    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n    x = layers.BatchNormalization(momentum=0.0)(x)\n    return layers.Activation(\"relu\")(x)\n\n\ndef dense_bn(x, filters):\n    x = layers.Dense(filters)(x)\n    x = layers.BatchNormalization(momentum=0.0)(x)\n    return layers.Activation(\"relu\")(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:48:33.460825Z","iopub.execute_input":"2024-07-01T20:48:33.461162Z","iopub.status.idle":"2024-07-01T20:48:33.467908Z","shell.execute_reply.started":"2024-07-01T20:48:33.461135Z","shell.execute_reply":"2024-07-01T20:48:33.466824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OrthogonalRegularizer(keras.regularizers.Regularizer):\n    def __init__(self, num_features, l2reg=0.001):\n        self.num_features = num_features\n        self.l2reg = l2reg\n        self.eye = ops.eye(num_features)\n\n    def __call__(self, x):\n        x = ops.reshape(x, (-1, self.num_features, self.num_features))\n        xxt = ops.tensordot(x, x, axes=(2, 2))\n        xxt = ops.reshape(xxt, (-1, self.num_features, self.num_features))\n        return ops.sum(self.l2reg * ops.square(xxt - self.eye))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:48:33.469268Z","iopub.execute_input":"2024-07-01T20:48:33.469571Z","iopub.status.idle":"2024-07-01T20:48:33.477560Z","shell.execute_reply.started":"2024-07-01T20:48:33.469546Z","shell.execute_reply":"2024-07-01T20:48:33.476583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tnet(inputs, num_features):\n    # Initialise bias as the identity matrix\n    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n    reg = OrthogonalRegularizer(num_features)\n\n    x = conv_bn(inputs, 32)\n    x = conv_bn(x, 64)\n    x = conv_bn(x, 512)\n    x = layers.GlobalMaxPooling1D()(x)\n    x = dense_bn(x, 256)\n    x = dense_bn(x, 128)\n    x = layers.Dense(\n        num_features * num_features,\n        kernel_initializer=\"zeros\",\n        bias_initializer=bias,\n        activity_regularizer=reg,\n    )(x)\n    feat_T = layers.Reshape((num_features, num_features))(x)\n    # Apply affine transformation to input features\n    return layers.Dot(axes=(2, 1))([inputs, feat_T])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:48:33.478730Z","iopub.execute_input":"2024-07-01T20:48:33.478989Z","iopub.status.idle":"2024-07-01T20:48:33.490198Z","shell.execute_reply.started":"2024-07-01T20:48:33.478967Z","shell.execute_reply":"2024-07-01T20:48:33.489276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = keras.Input(shape=(NUM_POINTS, 3))\n\nx = tnet(inputs, 3)\nx = conv_bn(x, 32)\nx = conv_bn(x, 32)\nx = tnet(x, 32)\nx = conv_bn(x, 32)\nx = conv_bn(x, 64)\nx = conv_bn(x, 512)\nx = layers.GlobalMaxPooling1D()(x)\nx = dense_bn(x, 256)\nx = layers.Dropout(0.3)(x)\nx = dense_bn(x, 128)\nx = layers.Dropout(0.3)(x)\n\noutputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:48:33.491374Z","iopub.execute_input":"2024-07-01T20:48:33.491664Z","iopub.status.idle":"2024-07-01T20:48:33.954930Z","shell.execute_reply.started":"2024-07-01T20:48:33.491639Z","shell.execute_reply":"2024-07-01T20:48:33.953978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    loss=\"sparse_categorical_crossentropy\",\n    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n    metrics=[\"sparse_categorical_accuracy\"],\n)\n\nhistory = model.fit(train_dataset, epochs=100, validation_data=validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:48:33.956163Z","iopub.execute_input":"2024-07-01T20:48:33.956483Z","iopub.status.idle":"2024-07-01T20:55:10.703832Z","shell.execute_reply.started":"2024-07-01T20:48:33.956457Z","shell.execute_reply":"2024-07-01T20:55:10.702768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"model.keras\")\nmodel.save(\"model2.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:55:10.705216Z","iopub.execute_input":"2024-07-01T20:55:10.705529Z","iopub.status.idle":"2024-07-01T20:55:11.284662Z","shell.execute_reply.started":"2024-07-01T20:55:10.705490Z","shell.execute_reply":"2024-07-01T20:55:11.283255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extracting the history of training and validation accuracy and loss\nacc = history.history['sparse_categorical_accuracy']\nval_acc = history.history['val_sparse_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\n# Creating plots\nplt.style.use('seaborn-whitegrid') # Use a modern style\ncolors = ['#377eb8', '#ff7f00']  # A set of colors: blue and orange\nlinestyles = ['-', '--']  # Solid and dashed lines\n\nfig = plt.figure(figsize=(12, 6))\nfig.patch.set_facecolor('#f7f7f7')\n\n# Plotting training and validation accuracy\nplt.subplot(1, 2, 1)\nplt.title('Training and Validation Accuracy', fontsize=20)\nplt.plot(epochs, acc, 'r', label='Training Accuracy', linestyle=linestyles[0], color=colors[0], linewidth=2)\nplt.plot(epochs, val_acc, 'b', label=\"Validation Accuracy\", linestyle=linestyles[1], color=colors[1], linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='best')\nplt.grid(True, linestyle='--', linewidth=0.2)\n\n# Plotting training and validation loss\nplt.subplot(1, 2, 2)\nplt.title('Training and Validation Loss', fontsize=20)\nplt.plot(epochs, loss, 'r', label='Training Loss', linestyle=linestyles[0], color=colors[0], linewidth=2)\nplt.plot(epochs, val_loss, 'b', label=\"Validation Loss\", linestyle=linestyles[1], color=colors[1], linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='best')\nplt.grid(True, linestyle='--', linewidth=0.2)\n\nplt.savefig(\"training-and-validation-accuracy-and-loss.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:55:11.285667Z","iopub.status.idle":"2024-07-01T20:55:11.286067Z","shell.execute_reply.started":"2024-07-01T20:55:11.285877Z","shell.execute_reply":"2024-07-01T20:55:11.285894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = test_dataset.take(1)\n\npoints, labels = list(data)[0]\npoints = points[:8, ...]\nlabels = labels[:8, ...]\n\n# run test data through model\npreds = model.predict(points)\npreds = ops.argmax(preds, -1)\n\npoints = points.numpy()\n\n# plot points with predicted class and label\nfig = plt.figure(figsize=(15, 10))\nfor i in range(8):\n    ax = fig.add_subplot(2, 4, i + 1, projection=\"3d\")\n    ax.scatter(points[i, :, 0], points[i, :, 1], points[i, :, 2])\n    ax.set_title(\n        \"pred: {:}, label: {:}\".format(\n            CLASS_MAP[preds[i].numpy()], CLASS_MAP[labels.numpy()[i]]\n        )\n    )\n    ax.set_axis_off()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:55:11.287255Z","iopub.status.idle":"2024-07-01T20:55:11.287620Z","shell.execute_reply.started":"2024-07-01T20:55:11.287440Z","shell.execute_reply":"2024-07-01T20:55:11.287467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = []  # store predicted labels\ny_true = []  # store true labels\n\nfor image_batch, label_batch in test_dataset:\n   # append true labels\n   y_true.append(label_batch)\n   # compute predictions\n   preds = model.predict(image_batch, verbose=0)\n   # append predicted labels\n   y_pred.append(np.argmax(preds, axis = - 1))\n\n# convert the true and predicted labels into tensors\ncorrect_labels = tf.concat([item for item in y_true], axis = 0)\npredicted_labels = tf.concat([item for item in y_pred], axis = 0)\n\nplt.figure(figsize=(30, 20))\nfig.patch.set_facecolor('#f7f7f7')\n\nax = sns.heatmap(metrics.confusion_matrix(correct_labels,predicted_labels), cmap=\"rocket_r\")\n\nax.set_title('Confusion Matrix', fontsize=20, pad=20)\nax.set_xlabel('Predicted Values', fontsize=14, labelpad=15)\nax.set_ylabel('Actual Values ', fontsize=14, labelpad=15)\n\nletters = \"ABCDEFGHIKLMNOPQRSTUVWXY\"\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(list(letters))\nax.yaxis.set_ticklabels(list(letters))\n\n# Improving tick labels for clarity\nax.tick_params(axis='x', labelsize=14)\nax.tick_params(axis='y', rotation=90, labelsize=14)\n\nplt.savefig(\"confusion-matrix-1.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:55:11.289843Z","iopub.status.idle":"2024-07-01T20:55:11.290695Z","shell.execute_reply.started":"2024-07-01T20:55:11.290399Z","shell.execute_reply":"2024-07-01T20:55:11.290421Z"},"trusted":true},"execution_count":null,"outputs":[]}]}